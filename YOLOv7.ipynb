{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f363b8",
   "metadata": {},
   "source": [
    "# Pose Estimation using YOLOv7 Algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dabdd1",
   "metadata": {},
   "source": [
    "`Previously, I worked on several computer vision and YOLOv3 projects like the automatic out-of-stock inventory management system. But, since YOLOv7 is considered as one of the best object detection algorithms, I gave it a try! Created in the year 2015, by Joseph Redmon (who also proposed YOLOv2 & YOLOv3). Since then, the open soure contributors have collaborated together to create more versions in the YOLO family like:`\n",
    "\n",
    "YOLOv4, YOLOv5, PP-YOLO, Scaled YOLOv4, PP-YOLOv2, YOLOv5, YOLOv6, and YOLOv7 (built on top of YOLOR - You Only Learn One Representation). YOLOv7 is more than just an object detection architecture. It provides a new model head that emits keypoints (skeleton) and can perform instance segmentation with just bounding box regression.\n",
    "\n",
    "Why did I use a YOLO model?\n",
    "1. Enables me to process video feeds at a high frames-per-second rate.\n",
    "\n",
    "2. Continuing R&D from the open-source community. YOLOv7 being the latest addition. \n",
    "\n",
    "3. No shortage of information or bug removals while implementing a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c716f",
   "metadata": {},
   "source": [
    "### Accuracy comparisons \n",
    "`1. YOLOv7-E6E        - 56.8 \n",
    " 2. Bi-Fusion Pyramid - 55.9 \n",
    " 3. Dual Swin R-CNN   - 53.9 \n",
    " 4. Faster-RCNN       - 42.0 \n",
    " 5. YOLOv6            - 35.0`\n",
    " \n",
    " Hence, our choice being YOLOv7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832b9e4",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f598ccce",
   "metadata": {},
   "source": [
    "## Downloading Necessary Weights\n",
    "Convolutional Neural Network (CNN) like the YOLO need a lot of images with variations to train on. To avoid redundancy, we transfer the learnings of the first few layers and just needs to learn the last (or maybe last few) layers to work for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5516eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -L https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt -o yolov7-w6-pose.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63aade1",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae61080",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c65faf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#For common image transformations\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "#Selects a subset of bounding boxes in descending order of score\n",
    "from utils.general import non_max_suppression_kpt\n",
    "\n",
    "#To resize and pad the video to a shape that the model can work with\n",
    "from utils.datasets import letterbox\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "\n",
    "#for graphs & plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for image and video analysis \n",
    "import cv2\n",
    "\n",
    "#for working with arrays\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4402311f",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ba451",
   "metadata": {},
   "source": [
    "## CPU or GPU availability check. Then, load pre-trained weights\n",
    "Having a GPU quickens the processing as it switches from float32 to float16. Peak float16 matrix multiplication and convolution performance is 16x faster than peak float32 performance on A100 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff79bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model():\n",
    "    model = torch.load('Weights/yolov7-w6-pose.pt', map_location=device)['model']\n",
    "    # Put in inference mode\n",
    "    model.float().eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # half() turns predictions into float16 tensors\n",
    "        # which significantly lowers inference time\n",
    "        model.half().to(device)\n",
    "    return model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a5d572",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fedc7",
   "metadata": {},
   "source": [
    "## Transform Video & Run through the model!\n",
    "Here, we enter an image as an array, from every frame of a video. Then, we resize and pad the video according to the model's requirement using letterbox() function, and run it through the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0285b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(image):\n",
    "    # Resize and pad image\n",
    "    image = letterbox(image, 960, stride=64, auto=True)[0] # shape: (567, 960, 3)\n",
    "    # Apply transforms\n",
    "    image = transforms.ToTensor()(image) # torch.Size([3, 567, 960])\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.half().to(device)\n",
    "    # Turn image into batch\n",
    "    image = image.unsqueeze(0) # torch.Size([1, 3, 567, 960])\n",
    "    with torch.no_grad():\n",
    "        output, _ = model(image)\n",
    "    return output, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e684c11d",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eb2832",
   "metadata": {},
   "source": [
    "## Returning Predictions & Image as a Tensor\n",
    "Returning image and predictions as a Tensor (which can store data in N dimensions, along with its linear operations). Then, we use the Non Maximum Suppression function, which is a computer vision method that selects a single entity out of many overlapping entities. Lastly, plotting the prediction skeletons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0217390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(output, image):\n",
    "    output = non_max_suppression_kpt(output, \n",
    "                                     0.25, # Confidence Threshold\n",
    "                                     0.65, # IoU Threshold\n",
    "                                     nc=model.yaml['nc'], # Number of Classes\n",
    "                                     nkpt=model.yaml['nkpt'], # Number of Keypoints\n",
    "                                     kpt_label=True)\n",
    "    with torch.no_grad():\n",
    "        output = output_to_keypoint(output)\n",
    "    nimg = image[0].permute(1, 2, 0) * 255\n",
    "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "    for idx in range(output.shape[0]):\n",
    "        plot_skeleton_kpts(nimg, output[idx, 7:].T, 3)\n",
    "\n",
    "    return nimg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984714db",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f2b323",
   "metadata": {},
   "source": [
    "## Using OpenCV to read a video:\n",
    "We run this entire process for every frame on a video. On each frame, we'll also write the frame into a new file, encoded as a video. This process will take significant time (more if done without a GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf37862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_estimation_video(filename):\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    # VideoWriter for saving the video\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    out = cv2.VideoWriter('Video_output.mp4', fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "    while cap.isOpened():\n",
    "        (ret, frame) = cap.read()\n",
    "        if ret == True:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            output, frame = run_inference(frame)\n",
    "            frame = draw_keypoints(output, frame)\n",
    "            frame = cv2.resize(frame, (int(cap.get(3)), int(cap.get(4))))\n",
    "            out.write(frame)\n",
    "            cv2.imshow('Pose estimation', frame)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b750801b",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a2d7c6",
   "metadata": {},
   "source": [
    "## Final Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19637b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr Juned bhai\\Conda\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "pose_estimation_video('Cap.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c5b7d",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c6127",
   "metadata": {},
   "source": [
    "## Then, the newly generated gets saved as 'Video_output' in the same directory! The last process will take some time, so be patient!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967db71a",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2eeee3",
   "metadata": {},
   "source": [
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
